{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ltwpbZMvRqt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from pathlib import Path\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()\n",
        "!kaggle datasets download -d vetrik/oasis-alzheimer\n",
        "!unzip oasis-alzheimer.zip -d ./oasis_alzheimer_data"
      ],
      "metadata": {
        "id": "IurQiV8Nh6NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMXNBJbCvRqt"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "973dGiClvRqu"
      },
      "outputs": [],
      "source": [
        "class Hparams:\n",
        "    def __init__(self, train_batch_size=64, test_batch_size=64, learning_rate=0.0005, num_epochs=10, val_split=0.1, test_split=0.1, model_path='saved_model', dataset_path='/content/oasis_alzheimer_data/content/Data', seed=42):\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.val_split = val_split\n",
        "        self.test_split = test_split\n",
        "        self.model_path = model_path\n",
        "        self.dataset_path = dataset_path\n",
        "        self.seed = seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz3TvCvI_n-Z"
      },
      "source": [
        "## Split data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ6xzhkv_tRg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import GroupKFold, train_test_split\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "        # Calculate mean and std\n",
        "        self.means = []\n",
        "        self.stds = []\n",
        "        for i in range(len(subset)):\n",
        "            x, _ = subset[i]\n",
        "            if self.transform:\n",
        "                x = self.transform(x)\n",
        "            self.means.append(torch.mean(x))\n",
        "            self.stds.append(torch.std(x))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.subset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, self.means[index], self.stds[index], y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "def get_transforms():\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((248, 248)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "def get_sample_weights(targets, indices):\n",
        "    \"\"\"Calculate sample weights for imbalanced classes\"\"\"\n",
        "    y_train = [targets[i] for i in indices]\n",
        "    class_sample_counts = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
        "    weights = 1. / class_sample_counts\n",
        "    sample_weights = np.array([weights[t] for t in y_train])\n",
        "    return torch.from_numpy(sample_weights).double()\n",
        "\n",
        "def get_subject_ids_from_paths(image_paths):\n",
        "    \"\"\"Extract subject IDs from image paths for OASIS dataset.\"\"\"\n",
        "    subject_ids = []\n",
        "    for path in image_paths:\n",
        "        # split the path into components\n",
        "        parts = path.split(os.path.sep)\n",
        "        # extract filename\n",
        "        filename = parts[-1]\n",
        "        # extract the subject ID from filename\n",
        "        subject_id = filename.split('_')[1]\n",
        "        subject_ids.append(subject_id)\n",
        "    return subject_ids\n",
        "\n",
        "def get_data_loaders(hparams):\n",
        "    # load dataset and get subject id\n",
        "    dataset = datasets.ImageFolder(hparams.dataset_path,\n",
        "                                 transform=transforms.Compose([transforms.Grayscale()]))\n",
        "\n",
        "    # get subject id\n",
        "    image_paths = [img[0] for img in dataset.imgs]\n",
        "    subject_ids = get_subject_ids_from_paths(image_paths)\n",
        "\n",
        "    # first split: separate test set by subjects\n",
        "    subjects = np.unique(subject_ids)\n",
        "    train_val_subjects, test_subjects = train_test_split(\n",
        "        subjects,\n",
        "        test_size=hparams.test_split,\n",
        "        random_state=hparams.seed\n",
        "    )\n",
        "\n",
        "    # second split: separate train/val from remaining subjects\n",
        "    train_subjects, val_subjects = train_test_split(\n",
        "        train_val_subjects,\n",
        "        test_size=hparams.val_split/(1-hparams.test_split),\n",
        "        random_state=hparams.seed\n",
        "    )\n",
        "\n",
        "    # create index masks based on subject split\n",
        "    train_indices = [i for i, subj in enumerate(subject_ids) if subj in train_subjects]\n",
        "    val_indices = [i for i, subj in enumerate(subject_ids) if subj in val_subjects]\n",
        "    test_indices = [i for i, subj in enumerate(subject_ids) if subj in test_subjects]\n",
        "\n",
        "    # create subsets\n",
        "    train_subset = torch.utils.data.Subset(dataset, train_indices)\n",
        "    val_subset = torch.utils.data.Subset(dataset, val_indices)\n",
        "    test_subset = torch.utils.data.Subset(dataset, test_indices)\n",
        "\n",
        "    print(f\"Total samples: {len(dataset)}\")\n",
        "    print(f\"Class distribution: {np.bincount(dataset.targets)}\")\n",
        "    print(f\"Training subjects: {len(train_subjects)}, Val subjects: {len(val_subjects)}, Test subjects: {len(test_subjects)}\")\n",
        "    print(f\"Train size: {len(train_subset)}, Val size: {len(val_subset)}, Test size: {len(test_subset)}\")\n",
        "\n",
        "    # apply transforms\n",
        "    data_transforms = get_transforms()\n",
        "\n",
        "    # create custom datasets with precomputed stats\n",
        "    train_dataset = CustomDataset(train_subset, transform=data_transforms)\n",
        "    val_dataset = CustomDataset(val_subset, transform=data_transforms)\n",
        "    test_dataset = CustomDataset(test_subset, transform=data_transforms)\n",
        "\n",
        "    # create samplers\n",
        "    sample_weights = get_sample_weights(dataset.targets, train_indices)\n",
        "    train_sampler = torch.utils.data.WeightedRandomSampler(\n",
        "        sample_weights,\n",
        "        len(sample_weights),\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "    # create data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=hparams.train_batch_size,\n",
        "        sampler=train_sampler,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=hparams.train_batch_size,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=hparams.test_batch_size\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA3J1MBuAt1C"
      },
      "source": [
        "Model ConvMixer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX_8aF4IArRz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class PatchExtractor(nn.Module):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, height, width = x.size()\n",
        "        assert height % self.patch_size == 0 and width % self.patch_size == 0, \\\n",
        "            \"Image dimensions must be divisible by the patch size\"\n",
        "\n",
        "        num_patches_h = height // self.patch_size\n",
        "        num_patches_w = width // self.patch_size\n",
        "        num_patches = num_patches_h * num_patches_w\n",
        "\n",
        "        # extract patches\n",
        "        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
        "        patches = patches.view(batch_size, num_patches, channels * self.patch_size * self.patch_size)\n",
        "        return patches\n",
        "\n",
        "\n",
        "class ConvMixerBlock(nn.Module):\n",
        "    def __init__(self, dim, kernel_size=9):\n",
        "        super().__init__()\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim, kernel_size, padding=\"same\", groups=dim),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm2d(dim),\n",
        "        )\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim * 4, kernel_size=1),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm2d(dim * 4),\n",
        "            nn.Conv2d(dim * 4, dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.residual(x)\n",
        "        x = x + residual\n",
        "        x = x + self.mlp(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvMixer(nn.Module):\n",
        "    def __init__(self, num_classes=4, patch_size=16, dim=256, depth=8, image_size=240):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = nn.Conv2d(1, dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.convmixer_blocks = nn.Sequential(*[ConvMixerBlock(dim) for _ in range(depth)])\n",
        "        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(dim, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, img, mean, std):\n",
        "        x = self.patch_embedding(img)\n",
        "        x = self.convmixer_blocks(x)\n",
        "        x = self.pooling(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device, epoch, num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    batch_size = 0\n",
        "\n",
        "    targets, preds = [], []\n",
        "\n",
        "    for batch_idx, (img, mean, std, target) in train_loader:\n",
        "        img, mean, std, target = img.to(device), mean.to(device), std.to(device), target.to(device)\n",
        "        batch_size = len(img)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(img, mean, std)\n",
        "        loss = criterion(output, target)\n",
        "        train_loss += loss.item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "        targets.append(target.cpu().numpy())\n",
        "        preds.append(pred.cpu().numpy().flatten())\n",
        "\n",
        "        train_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loader.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        train_loader.set_postfix(loss=train_loss / ((batch_idx+1) * len(img)), accuracy=100. * train_correct / ((batch_idx+1) * len(img)))\n",
        "\n",
        "    targets = np.concatenate(targets)\n",
        "    preds = np.concatenate(preds)\n",
        "    f1 = f1_score(targets, preds, average='macro')\n",
        "\n",
        "    train_length = train_loader.total * batch_size\n",
        "    train_loss /= train_length\n",
        "    train_accuracy = 100. * train_correct / train_length\n",
        "    return train_loss, train_accuracy, f1\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    total_size = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (img, mean, std, target) in enumerate(val_loader):\n",
        "            img, mean, std, target = img.to(device), mean.to(device), std.to(device), target.to(device)\n",
        "            batch_size = len(img)\n",
        "            output = model(img, mean, std)\n",
        "            loss = criterion(output, target)\n",
        "            val_loss += loss.item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            val_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "            total_size += len(img)\n",
        "    val_loss /= total_size\n",
        "    val_accuracy = 100. * val_correct / total_size\n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "\n",
        "def predict(model, data_loader, criterion, device, eval=False):\n",
        "    model.eval()\n",
        "    pred_loss = 0\n",
        "    pred_correct = 0\n",
        "    total_size = 0\n",
        "\n",
        "    predictions = torch.IntTensor()\n",
        "    ground_truths = torch.IntTensor()\n",
        "\n",
        "    predictions, ground_truths = predictions.to(device), ground_truths.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (img, mean, std, target) in enumerate(data_loader):\n",
        "            img, mean, std, target = img.to(device), mean, std.to(device), target.to(device)\n",
        "            output = model(img, mean, std)\n",
        "            loss = criterion(output, target)\n",
        "            pred_loss += loss.item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            pred_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "            predictions = torch.cat((predictions, pred), dim=0)\n",
        "            ground_truths = torch.cat((ground_truths, target), dim=0)\n",
        "\n",
        "            total_size += len(img)\n",
        "\n",
        "    pred_loss /= total_size\n",
        "    pred_accuracy = 100. * pred_correct / total_size\n",
        "\n",
        "    if eval:\n",
        "        return pred_loss, pred_accuracy, predictions.cpu().numpy(), ground_truths.cpu().numpy()\n",
        "    else:\n",
        "        return predictions.cpu().numpy(), ground_truths.cpu().numpy()\n",
        "\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, early_stopping=None):\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        loop = tqdm(enumerate(train_loader), total=len(train_loader), ascii=' >=')\n",
        "        train_loss, train_accuracy, f1 = train(model, loop, criterion, optimizer, device, epoch, num_epochs)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        if early_stopping is not None:\n",
        "            early_stopping(val_accuracy)\n",
        "\n",
        "            if early_stopping.early_stop:\n",
        "                tqdm.write(f'\\t => train_f1={f1:.4f}, val_loss={val_loss:.4f}, val_acc={val_accuracy:.4f}')\n",
        "                print(f'Early stopping at Epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "        tqdm.write(f'\\t => train_f1={f1:.4f}, val_loss={val_loss:.4f}, val_acc={val_accuracy:.4f}')\n",
        "\n",
        "    plot_losses(train_losses, val_losses)\n",
        "    plot_accuracies(train_accuracies, val_accuracies)\n",
        "\n",
        "    return train_losses, train_accuracies, val_losses, val_accuracies\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, mode='max'):\n",
        "        self.counter = 0\n",
        "        self.patience = patience\n",
        "        self.early_stop = False\n",
        "        self.mode = mode\n",
        "\n",
        "        if self.mode == 'max':\n",
        "            self.ref_value = float('-inf')\n",
        "        elif self.mode == 'min':\n",
        "            self.ref_value = float('inf')\n",
        "        else:\n",
        "            raise Exception(f'Undefined mode for EarlyStopping - mode: {mode}\\n'\n",
        "                             'Available modes are [\"max\", \"min\"]')\n",
        "\n",
        "    def __call__(self, value):\n",
        "        if self.mode == 'max':\n",
        "            if value <= self.ref_value:\n",
        "                self.counter += 1\n",
        "            else:\n",
        "                self.counter = 0\n",
        "                self.ref_value = value\n",
        "        elif self.mode == 'min':\n",
        "            if value >= self.ref_value:\n",
        "                self.counter += 1\n",
        "            else:\n",
        "                self.counter = 0\n",
        "                self.ref_value = value\n",
        "\n",
        "        if self.counter == self.patience:\n",
        "            self.early_stop = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTsInP3WvRqw"
      },
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbSRhWp6vRqw"
      },
      "outputs": [],
      "source": [
        "def plot_losses(train_losses, val_losses):\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracies(train_accuracies, val_accuracies):\n",
        "    plt.plot(train_accuracies, label='Training Accuracy')\n",
        "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo3FYBbDvRqw"
      },
      "source": [
        "## Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aI4Ntjq2vRqw"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device} device.\")\n",
        "hparams = Hparams()\n",
        "train_loader, val_loader, test_loader = get_data_loaders(hparams)\n",
        "early_stopping = EarlyStopping(patience=10, mode='max')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The train_loader contains {len(train_loader)} batches.\")\n",
        "print(f\"The val_loader contains {len(val_loader)} batches.\")\n",
        "print(f\"The test_loader contains {len(test_loader)} batches.\")"
      ],
      "metadata": {
        "id": "mT0yIHgIPpWP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfM-BV1PvRqw"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Opvb7HqvRqx"
      },
      "outputs": [],
      "source": [
        "model = ConvMixer().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTET1UOgvRqx"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=hparams.learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_HZFbZJnvRqx"
      },
      "outputs": [],
      "source": [
        "train_losses, train_accuracies, val_losses, val_accuracies = train_and_validate(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=hparams.num_epochs, early_stopping=early_stopping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zvBXvIsfvRqx"
      },
      "outputs": [],
      "source": [
        "plot_losses(train_losses, val_losses)\n",
        "plot_accuracies(train_accuracies, val_accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7tWszU3vRqx"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fnqNUts2vRqx"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = validate(model, test_loader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_confusion_matrix(model, data_loader, device, class_names):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, _, _, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images, None, None)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    # generate classification report\n",
        "    print(classification_report(all_targets, all_preds, target_names=class_names))\n",
        "\n",
        "    # generate confusion matrix\n",
        "    cm = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "    # calculate sensitivity for each class\n",
        "    sensitivity = []\n",
        "    for i in range(len(class_names)):\n",
        "        tp = cm[i, i]\n",
        "        fn = np.sum(cm[i, :]) - tp\n",
        "        sens = tp / (tp + fn) if (tp + fn) > 0 else 0  # handle division by zero\n",
        "        sensitivity.append(sens)\n",
        "        print(f\"Sensitivity for class {class_names[i]}: {sens:.4f}\")\n",
        "\n",
        "    # convert the confusion matrix to percentages\n",
        "    cm_percent = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]) * 100\n",
        "\n",
        "    df_cm = pd.DataFrame(cm_percent, index=class_names, columns=class_names)\n",
        "\n",
        "    # plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sn.heatmap(df_cm, annot=True, fmt='.1f', cmap='Blues',\n",
        "               annot_kws={\"size\": 12}, vmin=0, vmax=100, cbar=True)\n",
        "\n",
        "    plt.title('Normalized Confusion Matrix - ConvMixer', fontsize=16)\n",
        "    plt.xlabel('Predicted', fontsize=14)\n",
        "    plt.ylabel('Actual', fontsize=14, rotation=90, va=\"center\")\n",
        "    plt.xticks(rotation=0, ha='center', fontsize=12)\n",
        "    plt.yticks(rotation=0, fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# extract class names from the dataset\n",
        "class_names = train_loader.dataset.subset.dataset.classes\n",
        "\n",
        "create_confusion_matrix(model, test_loader, device, class_names)"
      ],
      "metadata": {
        "id": "DkDMRQkPNYs4"
      },
      "execution_count": 5,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}